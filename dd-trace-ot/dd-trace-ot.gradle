plugins {
  id "com.github.johnrengelman.shadow" version "5.2.0"
  id "me.champeau.gradle.jmh" version "0.5.0"
  id "com.palantir.revapi" version "1.4.0"
}

description = 'dd-trace-ot'

apply from: "$rootDir/gradle/java.gradle"
apply from: "$rootDir/gradle/publish.gradle"

// TODO raise these when equals() and hashCode() are excluded
minimumBranchCoverage = 0.5
minimumInstructionCoverage = 0.5

excludedClassesCoverage += [
  // This is mainly equals() and hashCode()
  "datadog.opentracing.OTSpanContext",
  // The builder is generated
  "datadog.opentracing.DDTracer.DDTracerBuilder"
]

apply plugin: 'org.unbroken-dome.test-sets'

testSets {
  ot31CompatabilityTest
  ot33CompatabilityTest
}

dependencies {
  annotationProcessor deps.autoservice
  implementation deps.autoservice

  compile project(':dd-trace-api')
  compile project(':dd-trace-core')

  // OpenTracing
  compile group: 'io.opentracing', name: 'opentracing-api', version: '0.32.0'
  compile group: 'io.opentracing', name: 'opentracing-noop', version: '0.32.0'
  compile group: 'io.opentracing', name: 'opentracing-util', version: '0.32.0'
  compile group: 'io.opentracing.contrib', name: 'opentracing-tracerresolver', version: '0.1.0'

  compile deps.slf4j
  compile deps.okio

  // We have autoservices defined in test subtree, looks like we need this to be able to properly rebuild this
  testAnnotationProcessor deps.autoservice
  testImplementation deps.autoservice

  testCompile project(":dd-java-agent:testing")
  testCompile group: 'com.github.stefanbirkner', name: 'system-rules', version: '1.19.0'

  ot31CompatabilityTestCompile group: 'io.opentracing', name: 'opentracing-api', version: '0.31.0'
  ot31CompatabilityTestCompile group: 'io.opentracing', name: 'opentracing-util', version: '0.31.0'
  ot31CompatabilityTestCompile group: 'io.opentracing', name: 'opentracing-noop', version: '0.31.0'

  ot33CompatabilityTestCompile group: 'io.opentracing', name: 'opentracing-api', version: '0.33.0'
  ot33CompatabilityTestCompile group: 'io.opentracing', name: 'opentracing-util', version: '0.33.0'
  ot33CompatabilityTestCompile group: 'io.opentracing', name: 'opentracing-noop', version: '0.33.0'
}

[
  configurations.ot31CompatabilityTestCompile,
  configurations.ot31CompatabilityTestRuntime
].each {
  it.resolutionStrategy {
    force group: 'io.opentracing', name: 'opentracing-api', version: '0.31.0'
    force group: 'io.opentracing', name: 'opentracing-util', version: '0.31.0'
    force group: 'io.opentracing', name: 'opentracing-noop', version: '0.31.0'
  }
}
[
  configurations.ot33CompatabilityTestCompile,
  configurations.ot33CompatabilityTestRuntime
].each {
  it.resolutionStrategy {
    force group: 'io.opentracing', name: 'opentracing-api', version: '0.33.0'
    force group: 'io.opentracing', name: 'opentracing-util', version: '0.33.0'
    force group: 'io.opentracing', name: 'opentracing-noop', version: '0.33.0'
  }
}

test.finalizedBy ot31CompatabilityTest
test.finalizedBy ot33CompatabilityTest

jar {
  archiveClassifier = 'unbundled'
}

def bundledProjects = [
  ':dd-trace-core',
  ':internal-api',
  ':utils:thread-utils',
  ':utils:container-utils'
]

shadowJar {
  archiveClassifier = ''

  dependencies {
    bundledProjects.forEach {
      include(project(it))
    }
  }
}

modifyPom {
  // We're bundling the internal dependencies.  So we need to add the transitive dependencies
  // of those projects. Directly adding to the XML is the only way to prevent the shadowJar plugin
  // from removing them
  def bundledProjectNames = bundledProjects.collect { it.substring(it.lastIndexOf(":") + 1) }
  withXml({ XmlProvider provider ->
    Node dependencies = provider.asNode().dependencies[0]
    def addedDependencies = project.configurations.getByName("compile").dependencies.collect { it.name }
    addedDependencies.addAll(bundledProjectNames)
    bundledProjects.forEach { bundledProject ->
      project(bundledProject).configurations.getByName("compile").dependencies.forEach { transitiveDependency ->
        if (!addedDependencies.contains(transitiveDependency.name)) {
          def dependency = dependencies.appendNode("dependency")
          dependency.appendNode("groupId", transitiveDependency.group)
          dependency.appendNode("artifactId", transitiveDependency.name)
          dependency.appendNode("version", transitiveDependency.version)
          dependency.appendNode("scope", "compile")
          addedDependencies.add(transitiveDependency.name)
        }
      }
    }
  })

  dependencies.removeAll { bundledProjectNames.contains(it.artifactId) }
}

jmh {
  //  include = [".*URLAsResourceNameBenchmark"]
  //  include = ['some regular expression'] // include pattern (regular expression) for benchmarks to be executed
  //  exclude = ['some regular expression'] // exclude pattern (regular expression) for benchmarks to be executed
  iterations = 1 // Number of measurement iterations to do.
  benchmarkMode = ['thrpt', 'avgt', 'ss']
  // Benchmark mode. Available modes are: [Throughput/thrpt, AverageTime/avgt, SampleTime/sample, SingleShotTime/ss, All/all]
  batchSize = 1
  // Batch size: number of benchmark method calls per operation. (some benchmark modes can ignore this setting)
  fork = 1 // How many times to forks a single benchmark. Use 0 to disable forking altogether
  failOnError = false // Should JMH fail immediately if any benchmark had experienced the unrecoverable error?
  forceGC = false // Should JMH force GC between iterations?
  //  jvm = 'myjvm' // Custom JVM to use when forking.
  //  jvmArgs = ['Custom JVM args to use when forking.']
  //  jvmArgsAppend = ['Custom JVM args to use when forking (append these)']
  //  jvmArgsPrepend =[ 'Custom JVM args to use when forking (prepend these)']
  //  humanOutputFile = project.file("${project.buildDir}/reports/jmh/human.txt") // human-readable output file
  //  resultsFile = project.file("${project.buildDir}/reports/jmh/results.txt") // results file
  //  operationsPerInvocation = 10 // Operations per invocation.
  //  benchmarkParameters =  [:] // Benchmark parameters.
  //  profilers = ['stack'] // Use profilers to collect additional data. Supported profilers: [cl, comp, gc, stack, perf, perfnorm, perfasm, xperf, xperfasm, hs_cl, hs_comp, hs_gc, hs_rt, hs_thr]
  timeOnIteration = '1s' // Time to spend at each measurement iteration.
  //  resultFormat = 'CSV' // Result format type (one of CSV, JSON, NONE, SCSV, TEXT)
  //  synchronizeIterations = false // Synchronize iterations?
  //  threads = 2 // Number of worker threads to run with.
  //  threadGroups = [2,3,4] //Override thread group distribution for asymmetric benchmarks.
  //  timeout = '1s' // Timeout for benchmark iteration.
  timeUnit = 'us' // Output time unit. Available time units are: [m, s, ms, us, ns].
  //  verbosity = 'NORMAL' // Verbosity mode. Available modes are: [SILENT, NORMAL, EXTRA]
  warmup = '2s' // Time to spend at each warmup iteration.
  //  warmupBatchSize = 10 // Warmup batch size: number of benchmark method calls per operation.
  warmupForks = 1 // How many warmup forks to make for a single benchmark. 0 to disable warmup forks.
  warmupIterations = 1 // Number of warmup iterations to do.
  //  warmupMode = 'INDI' // Warmup mode for warming up selected benchmarks. Warmup modes are: [INDI, BULK, BULK_INDI].
  //  warmupBenchmarks = ['.*Warmup'] // Warmup benchmarks to include in the run in addition to already selected. JMH will not measure these benchmarks, but only use them for the warmup.

  //  zip64 = true // Use ZIP64 format for bigger archives
  jmhVersion = '1.23' // Specifies JMH version
  //  includeTests = true // Allows to include test sources into generate JMH jar, i.e. use it when benchmarks depend on the test classes.
  duplicateClassesStrategy = 'warn'
  // Strategy to apply when encountring duplicate classes during creation of the fat jar (i.e. while executing jmhJar task)
}
