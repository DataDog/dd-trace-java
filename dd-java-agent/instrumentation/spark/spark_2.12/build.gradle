plugins {
  id 'java-test-fixtures'
}

def sparkVersion = '2.4.0'
def scalaVersion = '2.12'

muzzle {
  pass {
    group = "org.apache.spark"
    module = "spark-sql_$scalaVersion"
    versions = "[$sparkVersion,)"
    assertInverse = true
  }
}
configurations.configureEach {
  resolutionStrategy.deactivateDependencyLocking()
}
apply from: "$rootDir/gradle/java.gradle"

addTestSuiteForDir('latestDepTest', 'test')
addTestSuite('test_spark24')
addTestSuite('test_spark32')

testJvmConstraints {
  // Hadoop does not behave correctly with OpenJ9 https://issues.apache.org/jira/browse/HADOOP-18174
  excludeJdk = ['SEMERU8', 'SEMERU11']

  // Spark does not support Java > 11 until 3.3.0 https://issues.apache.org/jira/browse/SPARK-33772
  maxJavaVersion = JavaVersion.VERSION_11
}

dependencies {
  implementation project(':dd-java-agent:instrumentation:spark')

  compileOnly group: 'org.apache.spark', name: "spark-core_$scalaVersion", version: "$sparkVersion"
  compileOnly group: 'org.apache.spark', name: "spark-sql_$scalaVersion", version: "$sparkVersion"

  testImplementation(testFixtures(project(":dd-java-agent:instrumentation:spark")))
  testImplementation group: 'org.apache.spark', name: "spark-core_$scalaVersion", version: "$sparkVersion"
  testImplementation group: 'org.apache.spark', name: "spark-sql_$scalaVersion", version: "$sparkVersion"
  testImplementation group: 'org.apache.spark', name: "spark-yarn_$scalaVersion", version: "$sparkVersion"

  test_spark24Implementation group: 'org.apache.spark', name: "spark-core_$scalaVersion", version: "2.4.8"
  test_spark24Implementation group: 'org.apache.spark', name: "spark-sql_$scalaVersion", version: "2.4.8"
  test_spark24Implementation group: 'org.apache.spark', name: "spark-yarn_$scalaVersion", version: "2.4.8"

  test_spark32Implementation group: 'org.apache.spark', name: "spark-core_$scalaVersion", version: "3.2.4"
  test_spark32Implementation group: 'org.apache.spark', name: "spark-sql_$scalaVersion", version: "3.2.4"
  test_spark32Implementation group: 'org.apache.spark', name: "spark-yarn_$scalaVersion", version: "3.2.4"
  // We do not support netty versions older than this because of a change to the number of parameters to the
  // PooledByteBufAllocator constructor. See this PR where the new constructor (the only one we support) was introduced:
  // https://github.com/netty/netty/pull/10267
  test_spark32Implementation group: 'io.netty', name: 'netty-buffer', version: '4.1.52.Final'

  latestDepTestImplementation group: 'org.apache.spark', name: "spark-core_$scalaVersion", version: '+'
  latestDepTestImplementation group: 'org.apache.spark', name: "spark-sql_$scalaVersion", version: '+'
  latestDepTestImplementation group: 'org.apache.spark', name: "spark-yarn_$scalaVersion", version: '+'
}

tasks.named("test", Test) {
  dependsOn "test_spark24"
  dependsOn "test_spark32"
}
