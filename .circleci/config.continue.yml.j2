version: 2.1

defaults: &defaults
  working_directory: ~/dd-trace-java
  docker:
    - image: &default_container << pipeline.parameters.docker_image >>:<< pipeline.parameters.docker_image_tag >>

test_matrix: &test_matrix
  parameters:
    testJvm:
{% for jdk in nocov_jdks %}
      - "{{ jdk }}"
{% endfor %}

profiling_test_matrix: &profiling_test_matrix
  parameters:
    testJvm:
{% for jdk in all_jdks %}
      - "{{ jdk }}"
{% endfor %}

system_test_matrix: &system_test_matrix
  parameters:
    weblog-variant: [ 'spring-boot', 'spring-boot-jetty', 'spring-boot-openliberty', 'spring-boot-3-native', 'jersey-grizzly2', 'resteasy-netty3','ratpack', 'vertx3' ]

agent_integration_tests_modules: &agent_integration_tests_modules "dd-trace-core|communication|internal-api|utils"
core_modules: &core_modules "dd-java-agent|dd-trace-core|communication|internal-api|telemetry|utils|dd-java-agent/agent-bootstrap|dd-java-agent/agent-installer|dd-java-agent/agent-tooling|dd-java-agent/agent-builder|dd-java-agent/appsec|dd-java-agent/agent-crashtracking"
instrumentation_modules: &instrumentation_modules "dd-java-agent/instrumentation|dd-java-agent/agent-tooling|dd-java-agent/agent-installer|dd-java-agent/agent-builder|dd-java-agent/agent-bootstrap|dd-java-agent/appsec|dd-java-agent/testing|dd-trace-core|dd-trace-api|internal-api"
debugger_modules: &debugger_modules "dd-java-agent/agent-debugger|dd-java-agent/agent-bootstrap|dd-java-agent/agent-builder|internal-api|communication|dd-trace-core"
profiling_modules: &profiling_modules "dd-java-agent/agent-profiling"

default_system_tests_commit: &default_system_tests_commit 3598f2583cd5c25326cd7a5c01097374d94c1993

parameters:
  nightly:
    type: boolean
    default: false
  weekly:
    type: boolean
    default: false

  gradle_flags:
    # Using no-daemon is important for the caches to be in a consistent state
    type: string
    default: "--stacktrace --no-daemon"

  global_pattern:
    # Pattern for files that should always trigger a test jobs
    type: string
    default: "^build.gradle$|^settings.gradle$|^gradle.properties$|^buildSrc/|^gradle/|.circleci"

  docker_image:
    type: string
    default: ghcr.io/datadog/dd-trace-java-docker-build

  docker_image_tag:
    type: string
    default: base

commands:
  check_for_leftover_files:
    steps:
      - run:
          name: Check for leftover files
          command: |
            LEFTOVER_FILES=$(find . -type f -regex '.*\.orig$')
            if [[ "$LEFTOVER_FILES" != "" ]]
            then
              echo -e "Found leftover files in the commit:\n$LEFTOVER_FILES"
              exit 1
            fi

  generate_cache_ids:
    steps:
      - run:
          name: Generate cache ids
          command: |
            # Everything falls back to the main cache
            BASE_CACHE_ID="main"
            if [ "$CIRCLE_BRANCH" == "master" ];
            then
              # If we're on a the main branch, then they are the same
              echo "${BASE_CACHE_ID}" >| _circle_ci_cache_id
            else
              # If we're on a PR branch, then we use the name of the branch and the
              # PR number as a stable identifier for the branch cache
              echo "${CIRCLE_BRANCH}-${CIRCLE_PULL_REQUEST##*/}" >| _circle_ci_cache_id
            fi
            # Have new branches start from the main cache
            echo "${BASE_CACHE_ID}" >| _circle_ci_cache_base_id

  setup_code:
    steps:
      - checkout
      - run:
          name: Checkout merge commit
          command: |
            CCI_PR_NUMBER="${CIRCLE_PR_NUMBER:-${CIRCLE_PULL_REQUEST##*/}}"

            if [[ "$CIRCLE_BRANCH" != "master" && -n "${CCI_PR_NUMBER}" ]]
            then
              FETCH_REFS="${FETCH_REFS} +refs/pull/${CCI_PR_NUMBER}/merge:refs/pull/${CCI_PR_NUMBER}/merge +refs/pull/${CCI_PR_NUMBER}/head:refs/pull/${CCI_PR_NUMBER}/head"
              git fetch -u origin ${FETCH_REFS}

              if git merge-base --is-ancestor $(git show-ref --hash refs/pull/${CCI_PR_NUMBER}/head) $(git show-ref --hash refs/pull/${CCI_PR_NUMBER}/merge); then
                git checkout "pull/${CCI_PR_NUMBER}/merge"
              else
                echo "[WARN] There is a merge conflict between master and PR ${CCI_PR_NUMBER}, merge branch cannot be checked out."
                git checkout "pull/${CCI_PR_NUMBER}/head"
              fi
            fi

      - check_for_leftover_files

      - generate_cache_ids

  setup_testcontainers:
    description: >-
      Sets up remote docker and automatic port forwarding needed for docker on docker
      version of Testcontainers.
    steps:
      - setup_remote_docker:
          version: 20.10.18
          # DLC shares Docker layers across jobs (at an extra cost).
          # But its time to setup (~1min) exceeds the time required to prefetch all images we use.
          docker_layer_caching: false

      - run:
          name: Prepare testcontainers environment
          command: .circleci/prepare_docker_env.sh

      - run:
          name: Testcontainers tunnels
          background: true
          command: .circleci/start_docker_autoforward.sh

      - run:
          name: Prefetch Docker images
          background: true
          command: .circleci/fetch_docker_images.sh

  early_return_for_forked_pull_requests:
    description: >-
      If this build is from a fork, stop executing the current job and return success.
      This is useful to avoid steps that will fail due to missing credentials.
    steps:
      - run:
          name: Early return if this build is from a forked PR
          command: |
            if [[ "$CIRCLE_BRANCH" != "master" && -n "$CIRCLE_PR_NUMBER" ]]; then
              echo "Nothing to do for forked PRs, so marking this step successful"
              circleci step halt
            fi

  skip_unless_matching_files_changed:
    description: >-
      If files matching the regular expression haven't changed in the commit, then skip the job
    parameters:
      pattern:
        type: string
    steps:
      - run:
          name: "Check if files relevant to job have changed"
          command: |
            CCI_PR_NUMBER="${CIRCLE_PR_NUMBER:-${CIRCLE_PULL_REQUEST##*/}}"

            if [[ "$CIRCLE_BRANCH" != "master" && -n "$CCI_PR_NUMBER" ]]; then
              BRANCH="$(git rev-parse --abbrev-ref HEAD)"
              if [[ "$BRANCH" != "master" ]] && [[ "$BRANCH" != "release/*" ]]; then
                # We know that we have checked out the PR merge branch, so the HEAD commit is a merge
                # As a backup, if anything goes wrong with the diff, the build will fail
                CHANGED_FILES=$(git show HEAD | grep -e "^Merge:" | cut -d ' ' -f 2- | sed 's/ /.../' | xargs git diff --name-only)
                # Count the number of matches, and ignore if the grep doesn't match anything
                MATCH_COUNT=$(echo "$CHANGED_FILES" | grep -c -E "<< pipeline.parameters.global_pattern >>|<< parameters.pattern >>") || true
                if [[ "$MATCH_COUNT" -eq "0" ]]; then
                  circleci step halt
                fi
              fi
            fi

  display_memory_usage:
    steps:
      - run:
          name: Max Memory Used
          # The file does not seem to exist when DLC is disabled
          command: cat /sys/fs/cgroup/memory/memory.max_usage_in_bytes || true
          when: always


  # The caching setup of the build dependencies is somewhat involved because of how CircleCI works.
  # 1) Caches are immutable, so you can not reuse a cache key (the save will simply be ignored)
  # 2) Cache keys are prefix matched, and the most recently updated cache that matches will be picked
  #
  # There is a weekly job that runs on Monday mornings that builds a new cache from scratch.
  {% raw %}
  restore_dependency_cache:
    parameters:
      cacheType:
        type: string
    steps:
      - restore_cache:
          keys:
            # Dependent steps will find this cache
            - dd-trace-java-dep<< parameters.cacheType >>-v4-{{ .Branch }}-{{ checksum "_circle_ci_cache_id" }}-{{ .Revision }}
            # New branch commits will find this cache
            - dd-trace-java-dep<< parameters.cacheType >>-v4-{{ .Branch }}-{{ checksum "_circle_ci_cache_id" }}-
            # New branches fall back on main build caches
            - dd-trace-java-dep<< parameters.cacheType >>-v4-master-{{ checksum "_circle_ci_cache_base_id" }}-
            # Fallback to the previous cache during transition
            - dd-trace-java-dep<< parameters.cacheType >>-v3-master-{{ checksum "_circle_ci_cache_base_id" }}-

  save_dependency_cache:
    parameters:
      cacheType:
        type: string
    steps:
      - save_cache:
          key: dd-trace-java-dep<< parameters.cacheType >>-v4-{{ .Branch }}-{{ checksum "_circle_ci_cache_id" }}-{{ .Revision }}
          paths:
            # Cached dependencies and wrappers for gradle
            - ~/.gradle/caches
            - ~/.gradle/wrapper
            # Cached dependencies for maven
            - ~/.m2
            # Cached launchers and compilers for sbt
            - ~/.sbt
            # Cached dependencies for sbt handled by ivy
            - ~/.ivy2
            # Cached dependencies for sbt handled by coursier
            - ~/.cache/coursier

  restore_build_cache:
    parameters:
      cacheType:
        type: string
    steps:
      - restore_cache:
          keys:
            # Dependent steps will find this cache
            - dd-trace-java-build<< parameters.cacheType >>-v4-{{ .Branch }}-{{ checksum "_circle_ci_cache_id" }}-{{ .Revision }}

  save_build_cache:
    parameters:
      cacheType:
        type: string
    steps:
      - save_cache:
          key: dd-trace-java-build<< parameters.cacheType >>-v4-{{ .Branch }}-{{ checksum "_circle_ci_cache_id" }}-{{ .Revision }}
          paths:
            # Gradle version specific cache for incremental builds. Needs to match version in
            # gradle/wrapper/gradle-wrapper.properties
            - ~/.gradle/caches/8.3
            # Workspace
            - ~/dd-trace-java/.gradle
            - ~/dd-trace-java/workspace
{% endraw %}

  setup_system_tests:
    parameters:
      systemTestsCommit:
        type: string
        default: *default_system_tests_commit
    steps:
      - generate_cache_ids

      - restore_build_cache:
          cacheType: lib

      - run:
          name: Install python 3.9
          command: |
            sudo apt-get install python3.9-full python3.9-dev python3.9-venv
            echo 'export PATH="$HOME/.local/bin:$PATH"' >>"$BASH_ENV"

      - run:
          name: Clone system-tests
          command: |
            git init system-tests
            cd system-tests
            git remote add origin https://github.com/DataDog/system-tests.git
            git fetch origin << parameters.systemTestsCommit >>
            git reset --hard FETCH_HEAD

jobs:
  build:
    <<: *defaults
    resource_class: xlarge

    parameters:
      gradleTarget:
        type: string
      cacheType:
        type: string
      collectLibs:
        type: boolean
        default: false

    steps:
      - setup_code

      - restore_dependency_cache:
          cacheType: << parameters.cacheType >>

      - run:
          name: Build Project
          command: >-
            MAVEN_OPTS="-Xms64M -Xmx256M"
            GRADLE_OPTS="-Dorg.gradle.jvmargs='-Xmx2G -Xms2G -XX:ErrorFile=/tmp/hs_err_pid%p.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp'"
            ./gradlew clean
            << parameters.gradleTarget >>
            -PskipTests
            << pipeline.parameters.gradle_flags >>
            --max-workers=8
            --rerun-tasks

      - when:
          condition:
            equal: [ true, << parameters.collectLibs >> ]
          steps:
            - run:
                name: Collect Libs
                when: always
                command: .circleci/collect_libs.sh
            - store_artifacts:
                path: ./libs

      - run:
          name: Collect reports
          when: on_fail
          command: .circleci/collect_reports.sh --destination ./check_reports --move

      - run:
          name: Delete reports
          when: on_success
          command: .circleci/collect_reports.sh --destination ./check_reports --delete

      - store_artifacts:
          path: ./check_reports

      # Save a full dependency cache when building on master or a base project branch.
      # We used to do this on the first build of each PR, but now it's skipped at the
      # cost of downloading new dependencies a few more times.
      - when:
          condition:
            matches:
              pattern: "^(master|project/.+)$"
              value: << pipeline.git.branch >>
          steps:
            - save_dependency_cache:
                cacheType: << parameters.cacheType >>

      # Save the small build cache
      - save_build_cache:
          cacheType: << parameters.cacheType >>

      - display_memory_usage

  spotless:
    <<: *defaults
    resource_class: medium+

    steps:
      - setup_code

      - run:
          name: Run spotless
          command: >-
            JAVA_HOME=$JAVA_11_HOME
            ./gradlew spotlessCheck
            << pipeline.parameters.gradle_flags >>
            --max-workers=8

  check:
    <<: *defaults

    parameters:
      parallelism:
        type: integer
        default: 1
      gradleTarget:
        type: string
      cacheType:
        type: string

    resource_class: medium+

    parallelism: << parameters.parallelism >>

    steps:
      - setup_code
      - restore_dependency_cache:
          cacheType: << parameters.cacheType >>
      - restore_build_cache:
          cacheType: << parameters.cacheType >>

      - run:
          name: Check Project
          command: >-
            MAVEN_OPTS="-Xms64M -Xmx256M"
            GRADLE_OPTS="-Dorg.gradle.jvmargs='-Xmx2G -Xms2G -XX:ErrorFile=/tmp/hs_err_pid%p.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp'"
            ./gradlew
            << parameters.gradleTarget >>
            -PskipTests
            -PrunBuildSrcTests
            -PtaskPartitionCount=${CIRCLE_NODE_TOTAL} -PtaskPartition=${CIRCLE_NODE_INDEX}
            << pipeline.parameters.gradle_flags >>
            --max-workers=8

      - run:
          name: Collect reports
          when: on_fail
          command: .circleci/collect_reports.sh --destination ./check_reports --move

      - run:
          name: Delete reports
          when: on_success
          command: .circleci/collect_reports.sh --destination ./check_reports --delete

      - store_artifacts:
          path: ./check_reports

      - run:
          name: Cancel workflow
          when: on_fail
          command: .circleci/cancel_workflow.sh

  build_clean_cache:
    <<: *defaults

    parameters:
      gradleTarget:
        type: string
      cacheType:
        type: string
      collectLibs:
        type: boolean
        default: false

    resource_class: xlarge

    steps:
      - setup_code

      - run:
          name: Build Project
          command: >-
            MAVEN_OPTS="-Xms64M -Xmx256M"
            GRADLE_OPTS="-Dorg.gradle.jvmargs='-Xmx2G -Xms2G -XX:ErrorFile=/tmp/hs_err_pid%p.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp'"
            ./gradlew clean
            << parameters.gradleTarget >>
            -PskipTests
            << pipeline.parameters.gradle_flags >>
            --max-workers=8
            --rerun-tasks

      - when:
          condition:
            not:
              equal: [true, << parameters.collectLibs >>]
          steps:
          - run:
              name: Collect Libs
              when: always
              command: .circleci/collect_libs.sh
          - store_artifacts:
              path: ./libs

      - save_dependency_cache:
          cacheType: << parameters.cacheType >>

      - display_memory_usage

  tests: &tests
    <<: *defaults
    # since tests use test containers, they will use a Linux VM / Remote Docker executor, so there is no medium+ size
    resource_class: large

    docker:
      - image: << pipeline.parameters.docker_image >>:<< parameters.testJvm >>

    parameters:
      testJvm:
        type: string
        default: ""
      maxDaemonHeapSize:
        type: string
        default: "2G"
      gradleParameters:
        type: string
        default: ""
      gradleTarget:
        type: string
      triggeredBy:
        type: string
        default: ".*"
      stage:
        type: string
        default: ""
      parallelism:
        type: integer
        default: 1
      maxWorkers:
        type: integer
        default: 2
      profile:
        type: boolean
        default: false
      continueOnFailure:
        type: boolean
        default: false
      cacheType:
        type: string

    parallelism: << parameters.parallelism >>

    steps:
      - setup_code

      - skip_unless_matching_files_changed:
          pattern: << parameters.triggeredBy >>

      - restore_dependency_cache:
          cacheType: << parameters.cacheType >>
      - restore_build_cache:
          cacheType: << parameters.cacheType >>

      - when:
          condition:
            or:
              - equal: ["core", << parameters.stage >>]
              - equal: ["instrumentation", << parameters.stage >>]
              - equal: ["smoke", << parameters.stage >>]
          steps:
            - setup_testcontainers

      - run:
          name: Run tests
          command: >-
            if [[ << parameters.profile >> ]] && [[ << parameters.testJvm >> != "ibm8" ]] && [[ << parameters.testJvm >> != "oracle8" ]];
            then
              PROFILER_COMMAND="-XX:StartFlightRecording=settings=profile,filename=/tmp/<< parameters.stage >>-<< parameters.testJvm >>.jfr,dumponexit=true"
            fi
            
            MAVEN_OPTS="-Xms64M -Xmx512M"
            GRADLE_OPTS="-Dorg.gradle.jvmargs='-Xms<< parameters.maxDaemonHeapSize >> -Xmx<< parameters.maxDaemonHeapSize >> $PROFILER_COMMAND -XX:ErrorFile=/tmp/hs_err_pid%p.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp' -Ddatadog.forkedMaxHeapSize=768M -Ddatadog.forkedMinHeapSize=128M"
            ./gradlew
            << parameters.gradleTarget >>
            << parameters.gradleParameters >>
            -PtaskPartitionCount=${CIRCLE_NODE_TOTAL} -PtaskPartition=${CIRCLE_NODE_INDEX}
            <<# parameters.testJvm >>-PtestJvm=<< parameters.testJvm >><</ parameters.testJvm >>
            << pipeline.parameters.gradle_flags >>
            --max-workers=<< parameters.maxWorkers >>
            --continue
            <<# parameters.continueOnFailure >> || true <</ parameters.continueOnFailure >>

      - run:
          name: Collect reports
          when: on_fail
          command: .circleci/collect_reports.sh

      - store_artifacts:
          path: ./reports.tar

      - when:
          condition:
            equal: [true, << parameters.profile >>]
          steps:
            - run:
                name: Collect profiles
                when: always
                command: .circleci/collect_profiles.sh

            - store_artifacts:
                path: ./profiles.tar

      - run:
          name: Collect test results
          when: always
          command: .circleci/collect_results.sh

      - store_test_results:
          path: ./results

      - display_memory_usage

      - early_return_for_forked_pull_requests

      - run:
          name: Upload test results to Datadog
          when: always
          command: .circleci/upload_ciapp.sh << parameters.stage >> << parameters.testJvm >> || true

      - run:
          name: Get APM Test Agent Trace Check Results
          when: always
          command: |
            set +e  # Disable exiting from testagent response failure
            SUMMARY_RESPONSE=$(curl -s -w "\n%{http_code}" -o summary_response.txt http://localhost:8126/test/trace_check/summary)
            set -e
            SUMMARY_RESPONSE_CODE=$(echo "$SUMMARY_RESPONSE" | awk 'END {print $NF}')

            if [[ SUMMARY_RESPONSE_CODE -eq 200 ]]; then
              echo "APM Test Agent is running. (HTTP 200)"
            else
              echo "APM Test Agent is not running and was not used for testing. No checks failed."
              exit 0
            fi
            
            RESPONSE=$(curl -s -w "\n%{http_code}" -o response.txt http://localhost:8126/test/trace_check/failures)
            RESPONSE_CODE=$(echo "$RESPONSE" | awk 'END {print $NF}')
               
            if [[ $RESPONSE_CODE -eq 200 ]]; then
              echo "All APM Test Agent Check Traces returned successful! (HTTP 200)"
              echo "APM Test Agent Check Traces Summary Results:"
              cat summary_response.txt | jq '.'
            elif [[ $RESPONSE_CODE -eq 404 ]]; then
              echo "Real APM Agent running in place of TestAgent, no checks to validate!"
            else
              echo "APM Test Agent Check Traces failed with response code: $RESPONSE_CODE"
              echo "Failures:"
              cat response.txt
              echo "APM Test Agent Check Traces Summary Results:"
              cat summary_response.txt | jq '.'
              exit 1
            fi

  xlarge_tests:
    <<: *tests

    docker:
      - image: << pipeline.parameters.docker_image >>:<< parameters.testJvm >>
        environment:
          - CI_USE_TEST_AGENT=true
      - image: ghcr.io/datadog/dd-apm-test-agent/ddapm-test-agent:v1.11.0
        environment:
          - LOG_LEVEL=DEBUG
          - TRACE_LANGUAGE=java
          - DD_SUPPRESS_TRACE_PARSE_ERRORS=true
          - DD_POOL_TRACE_CHECK_FAILURES=true
          - DD_DISABLE_ERROR_RESPONSES=true
          - ENABLED_CHECKS=trace_content_length,trace_stall,meta_tracer_version_header,trace_count_header,trace_peer_service,trace_dd_service
    resource_class: xlarge


  # The only way to do fan-in in CircleCI seems to have a proper job, so let's have one that
  # doesn't consume so many resources. The execution time for this including spin up seems to
  # be around 6 seconds.
  fan_in:
    resource_class: small

    docker:
      - image: alpine

    parameters:
      testJvm:
        type: string
        default: "all configured JVMs"
      stage:
        type: string

    steps:
      - run:
          name: Completed stage << parameters.stage >> on << parameters.testJvm >> passed!
          command: echo '<< parameters.stage >> completed!'

  agent_integration_tests:
    <<: *tests

    resource_class: medium

    docker:
      - image: << pipeline.parameters.docker_image >>:7
      - image: datadog/agent:7.34.0
        environment:
          - DD_APM_ENABLED=true
          - DD_BIND_HOST=0.0.0.0
          - DD_API_KEY=invalid_key_but_this_is_fine

  test_published_artifacts:
    <<: *defaults
    resource_class: medium
    docker:
      - image: << pipeline.parameters.docker_image >>:7

    steps:
      - setup_code
      - restore_dependency_cache:
          cacheType: lib
      - restore_build_cache:
          cacheType: lib

      - run:
          name: Publish Artifacts Locally
          command: |
            mvn_local_repo=$(./mvnw help:evaluate -Dexpression=settings.localRepository -q -DforceStdout)
            rm -rf "${mvn_local_repo}/com/datadoghq"
            export GRADLE_OPTS="-Dorg.gradle.jvmargs='-Xmx2G -Xms2G -XX:ErrorFile=/tmp/hs_err_pid%p.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp'"
            ./gradlew publishToMavenLocal << pipeline.parameters.gradle_flags >> --max-workers=3

      - run:
          name: Test Published Artifacts
          command: |
            cd test-published-dependencies
            export GRADLE_OPTS="-Dorg.gradle.jvmargs='-Xmx512M -Xms512M -XX:ErrorFile=/tmp/hs_err_pid%p.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp'"
            ./gradlew check --info --max-workers=3

      - run:
          name: Collect Reports
          when: on_fail
          command: .circleci/collect_reports.sh

      - store_artifacts:
          path: ./reports

      - display_memory_usage

  muzzle:
    <<: *defaults
    resource_class: medium
    parallelism: 3
    steps:
      - setup_code

      - skip_unless_matching_files_changed:
          pattern: "dd-java-agent/instrumentation"

      # We are not running with a separate cache of all muzzle artifacts here because it gets very big and
      # ends up taking more time restoring/saving than the actual increase in time it takes just
      # downloading the artifacts each time.
      #
      # Let's at least restore the build cache to have something to start from.
      - restore_dependency_cache:
          cacheType: inst
      - restore_build_cache:
          cacheType: inst

      - run:
          name: Gather muzzle tasks
          command: >-
            SKIP_BUILDSCAN="true"
            ./gradlew writeMuzzleTasksToFile
            << pipeline.parameters.gradle_flags >>
            --max-workers=3

      - run:
          name: Verify Muzzle
          command: >-
            SKIP_BUILDSCAN="true"
            GRADLE_OPTS="-Dorg.gradle.jvmargs='-Xmx2G -Xms2G -XX:ErrorFile=/tmp/hs_err_pid%p.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp'"
            ./gradlew `circleci tests split --split-by=timings workspace/build/muzzleTasks | xargs`
            << pipeline.parameters.gradle_flags >>
            --max-workers=4

      - run:
          name: Collect Reports
          when: on_fail
          command: .circleci/collect_reports.sh

      - store_artifacts:
          path: ./reports

      - store_test_results:
          path: workspace/build/muzzle-test-results

      - display_memory_usage

  system-tests:
    machine:
      # https://support.circleci.com/hc/en-us/articles/360007324514-How-can-I-use-Docker-volume-mounting-on-CircleCI-
      image: ubuntu-2004:current
    resource_class: medium
    parameters:
      weblog-variant:
        type: string
    steps:
      - setup_system_tests

      - run:
          name: Copy jar file to system test binaries folder
          command: |
            ls -la ~/dd-trace-java/workspace/dd-java-agent/build/libs
            cp ~/dd-trace-java/workspace/dd-java-agent/build/libs/*.jar system-tests/binaries/

      - run:
          name: Build
          command: |
            cd system-tests
            ./build.sh java --weblog-variant << parameters.weblog-variant >> 

      - run:
          name: Run
          command: |
            cd system-tests
            DD_API_KEY=$SYSTEM_TESTS_DD_API_KEY ./run.sh

      - run:
          name: Run APM E2E default tests
          # Stop the job after 5m to avoid excessive overhead. Will need adjustment as more tests are added.
          no_output_timeout: 5m
          command: |
            cd system-tests
            DD_SITE=datadoghq.com DD_API_KEY=$SYSTEM_TESTS_E2E_DD_API_KEY DD_APPLICATION_KEY=$SYSTEM_TESTS_E2E_DD_APP_KEY ./run.sh APM_TRACING_E2E

      - run:
          name: Run APM E2E Single Span tests
          # Stop the job after 5m to avoid excessive overhead. Will need adjustment as more tests are added.
          no_output_timeout: 5m
          command: |
            cd system-tests
            DD_SITE=datadoghq.com DD_API_KEY=$SYSTEM_TESTS_E2E_DD_API_KEY DD_APPLICATION_KEY=$SYSTEM_TESTS_E2E_DD_APP_KEY ./run.sh APM_TRACING_E2E_SINGLE_SPAN

      - run:
          name: Upload data to CI Visibility
          command: |
            cd system-tests
            export DD_API_KEY=$SYSTEM_TESTS_CI_API_KEY
            export DD_APP_KEY=$SYSTEM_TESTS_CI_APP_KEY

            # Causes conflicts with DD_API_KEY and datadog-ci tool
            unset DATADOG_API_KEY
            
            echo "Uploading tests results to CI Visibility"  
            utils/scripts/upload_results_CI_visibility.sh dev java-tracer << pipeline.id >>-<< pipeline.number >>

            if [[ $CIRCLE_BRANCH == "master" ]]; then
              echo "Updating dashboard from dd-trace-java main branch"
              utils/scripts/update_dashboard_CI_visibility.sh java-tracer << pipeline.id >>-<< pipeline.number >>
            else
              echo "Skipping CI Visibility dashboard update due to it is not a main branch"
            fi

      - run:
          name: Collect artifacts
          command: tar -cvzf logs_java_<< parameters.weblog-variant >>_dev.tar.gz -C system-tests logs logs_apm_tracing_e2e logs_apm_tracing_e2e_single_span

      - store_artifacts:
          path: logs_java_<< parameters.weblog-variant >>_dev.tar.gz

  integrations-system-tests:
    machine:
      # https://support.circleci.com/hc/en-us/articles/360007324514-How-can-I-use-Docker-volume-mounting-on-CircleCI-
      image: ubuntu-2004:current
    resource_class: medium
    steps:
      - setup_system_tests

      - run:
          name: Copy jar file to system test binaries folder
          command: |
            ls -la ~/dd-trace-java/workspace/dd-java-agent/build/libs
            cp ~/dd-trace-java/workspace/dd-java-agent/build/libs/*.jar system-tests/binaries/

      - run:
          name: Build
          # Build the default framework, which is springboot
          command: |
            cd system-tests
            ./build.sh java

      - run:
          name: Run APM Integrations tests
          # Stop the job after 5m to avoid excessive overhead. Will need adjustment as more tests are added.
          no_output_timeout: 5m
          command: |
            cd system-tests
            DD_SITE=datadoghq.com DD_API_KEY=$SYSTEM_TESTS_E2E_DD_API_KEY DD_APPLICATION_KEY=$SYSTEM_TESTS_E2E_DD_APP_KEY ./run.sh INTEGRATIONS

      - store_test_results:
          path: system-tests/logs_integrations

      - store_artifacts:
          path: system-tests/logs_integrations

  parametric-tests:
    machine:
      # https://support.circleci.com/hc/en-us/articles/360007324514-How-can-I-use-Docker-volume-mounting-on-CircleCI-
      image: ubuntu-2004:current
    resource_class: large
    steps:
      - setup_system_tests

      - run:
          name: Copy jar files to system test binaries folder
          command: |
            ls -la ~/dd-trace-java/workspace/dd-trace-api/build/libs
            ls -la ~/dd-trace-java/workspace/dd-java-agent/build/libs
            cp ~/dd-trace-java/workspace/dd-trace-api/build/libs/*.jar system-tests/binaries/
            cp ~/dd-trace-java/workspace/dd-java-agent/build/libs/*.jar system-tests/binaries/

      - run:
          name: Install requirements
          command: |
            cd system-tests
            pyenv local system
            python3.9 --version
            python3.9 -m pip install wheel
            python3.9 -m pip install -r requirements.txt
            sudo ln -sf /usr/bin/python3.9 /usr/bin/python

      - run:
          name: Run
          command: |
            set -e
            cd system-tests
            export TEST_LIBRARY=java
            export PYTEST_WORKER_COUNT=8
            ./build.sh -i runner
            ./run.sh PARAMETRIC --log-cli-level=DEBUG --durations=30 -vv

      - store_test_results:
          path: system-tests/logs_parametric

      - run:
          name: Collect artifacts
          command: tar -cvzf logs_java_parametric_dev.tar.gz -C system-tests logs_parametric

      - store_artifacts:
          path: logs_java_parametric_dev.tar.gz


build_test_jobs: &build_test_jobs
  - build:
      name: build_lib
      gradleTarget: shadowJar
      cacheType: lib
      collectLibs: true
  - build:
      name: build_base
      gradleTarget: :baseTest
      cacheType: base
  - build:
      name: build_inst
      gradleTarget: :instrumentationTest
      cacheType: inst
  - build:
      name: build_latestdep
      gradleTarget: :instrumentationLatestDepTest
      cacheType: latestdep
  - build:
      name: build_smoke
      gradleTarget: :smokeTest
      cacheType: smoke
  - build:
      name: build_profiling
      gradleTarget: :profilingTest
      cacheType: profiling
  - spotless

  - fan_in:
      requires:
        - build_lib
        - build_base
        - build_inst
        - build_smoke
        - build_profiling
        - spotless
      name: ok_to_test
      stage: ok_to_test

  - check:
      requires:
        - ok_to_test
      name: check_base
      gradleTarget: ":baseCheck"
      cacheType: base

  - check:
      requires:
        - ok_to_test
      name: check_inst
      parallelism: 4
      gradleTarget: ":instrumentationCheck"
      cacheType: inst

  - check:
      requires:
        - ok_to_test
      name: check_smoke
      gradleTarget: ":smokeCheck"
      cacheType: smoke

  - check:
      requires:
        - ok_to_test
      name: check_profiling
      gradleTarget: ":profilingCheck"
      cacheType: profiling

  - fan_in:
      requires:
        - check_base
        - check_inst
        - check_smoke
        - check_profiling
      name: check
      stage: check

  - tests:
      requires:
        - ok_to_test
      name: z_test_<< matrix.testJvm >>_base
      triggeredBy: *core_modules
      gradleTarget: ":baseTest"
      gradleParameters: "-PskipFlakyTests -PskipInstTests -PskipSmokeTests -PskipProfilingTests"
      stage: core
      cacheType: base
      parallelism: 4
      maxWorkers: 4
      matrix:
        <<: *test_matrix

  - tests:
      requires:
        - ok_to_test
      name: z_test_8_base
      triggeredBy: *core_modules
      gradleTarget: :baseTest jacocoTestReport jacocoTestCoverageVerification
      gradleParameters: "-PskipFlakyTests -PskipInstTests -PskipSmokeTests -PskipProfilingTests"
      stage: core
      cacheType: base
      parallelism: 4
      maxWorkers: 4
      testJvm: "8"

  - xlarge_tests:
      requires:
        - ok_to_test
      name: z_test_<< matrix.testJvm >>_inst
      gradleTarget: ":instrumentationTest"
      gradleParameters: "-PskipFlakyTests"
      triggeredBy: *instrumentation_modules
      stage: instrumentation
      cacheType: inst
      parallelism: 4
      maxWorkers: 4
      matrix:
        <<: *test_matrix

  - xlarge_tests:
      requires:
        - ok_to_test
      name: z_test_8_inst
      gradleTarget: ":instrumentationTest"
      gradleParameters: "-PskipFlakyTests"
      triggeredBy: *instrumentation_modules
      stage: instrumentation
      cacheType: inst
      parallelism: 4
      maxWorkers: 4
      testJvm: "8"

  - xlarge_tests:
      requires:
        - ok_to_test
        - build_latestdep
      name: test_8_inst_latest
      gradleTarget: ":instrumentationLatestDepTest"
      gradleParameters: "-PskipFlakyTests"
      triggeredBy: *instrumentation_modules
      stage: instrumentation
      cacheType: latestdep
      parallelism: 4
      maxWorkers: 4
      testJvm: "8"

{% if flaky %}
  - tests:
      requires:
        - ok_to_test
      name: z_test_8_flaky_base
      gradleTarget: ":baseTest"
      gradleParameters: "-PrunFlakyTests"
      continueOnFailure: true
      triggeredBy: *core_modules
      stage: core
      cacheType: base
      parallelism: 4
      maxWorkers: 4
      testJvm: "8"

  - xlarge_tests:
      requires:
        - ok_to_test
      name: z_test_8_flaky_inst
      gradleTarget: ":instrumentationTest"
      gradleParameters: "-PrunFlakyTests"
      continueOnFailure: true
      triggeredBy: *instrumentation_modules
      stage: instrumentation
      cacheType: inst
      parallelism: 2
      maxWorkers: 4
      testJvm: "8"

  - tests:
      requires:
        - ok_to_test
      name: z_test_8_flaky_smoke
      gradleTarget: ":smokeTest"
      gradleParameters: "-PrunFlakyTests"
      continueOnFailure: true
      stage: smoke
      cacheType: smoke
      parallelism: 4
      maxWorkers: 4
      testJvm: "8"
{% endif %}

  - tests:
      requires:
        - ok_to_test
      maxWorkers: 4
      gradleTarget: ":profilingTest"
      gradleParameters: "-PskipFlakyTests"
      triggeredBy: *profiling_modules
      stage: profiling
      cacheType: profiling
      name: test_<< matrix.testJvm >>_profiling
      matrix:
        <<: *profiling_test_matrix

  - tests:
      requires:
        - ok_to_test
      name: test_<< matrix.testJvm >>_debugger
      maxWorkers: 4
      gradleTarget: ":debuggerTest"
      gradleParameters: "-PskipFlakyTests"
      triggeredBy: *debugger_modules
      stage: debugger
      cacheType: base
      matrix:
        <<: *profiling_test_matrix

  - tests:
      requires:
        - ok_to_test
      name: z_test_<< matrix.testJvm >>_smoke
      gradleTarget: "stageMainDist :smokeTest"
      gradleParameters: "-PskipFlakyTests"
      stage: smoke
      cacheType: smoke
      parallelism: 4
      maxWorkers: 3
      matrix:
        <<: *test_matrix

  - tests:
      requires:
        - ok_to_test
      name: test_graalvm17_smoke
      gradleTarget: "stageMainDist :dd-smoke-test:spring-boot-3.0-native:test"
      stage: smoke
      cacheType: smoke
      testJvm: "graalvm17"

  - tests:
      requires:
        - ok_to_test
      name: z_test_8_smoke
      gradleTarget: "stageMainDist :smokeTest"
      gradleParameters: "-PskipFlakyTests"
      stage: smoke
      cacheType: smoke
      parallelism: 4
      maxWorkers: 3
      testJvm: "8"

  - fan_in:
      requires:
        - z_test_<< matrix.testJvm >>_base
        - z_test_<< matrix.testJvm >>_inst
        - z_test_<< matrix.testJvm >>_smoke
      name: test_<< matrix.testJvm >>
      stage: tracing
      matrix:
        <<: *test_matrix

  - fan_in:
      requires:
        - z_test_8_base
        - z_test_8_inst
        - z_test_8_smoke
      name: test_8
      stage: tracing
      testJvm: "8"

  - agent_integration_tests:
      requires:
        - ok_to_test
      triggeredBy: *agent_integration_tests_modules
      gradleTarget: traceAgentTest
      cacheType: base
      testJvm: "8"

  - test_published_artifacts:
      requires:
        - ok_to_test

  - muzzle:
      requires:
        - ok_to_test
      filters:
        branches:
          ignore:
            - master
            - project/*
            - release/*

  - system-tests:
      requires:
        - ok_to_test
      matrix:
          <<: *system_test_matrix

  - integrations-system-tests:
      requires:
        - ok_to_test

  - parametric-tests:
      requires:
        - ok_to_test

  - fan_in:
      requires:
        - test_published_artifacts
{% for jdk in all_jdks %}
        - "test_{{ jdk }}_profiling"
{% endfor %}
      name: profiling
      stage: profiling

  - fan_in:
      requires:
        - test_published_artifacts
{% for jdk in all_jdks %}
        - "test_{{ jdk }}_debugger"
{% endfor %}
      name: debugger
      stage: debugger

  # This job requires all the jobs needed for a successful build, so GitHub only needs to enforce this one,
  # and it will be simpler to require different JVM versions for different branches and old releases
  - fan_in:
      requires:
        - check
        - test_published_artifacts
        - agent_integration_tests
{% for jdk in all_jdks %}
        - "test_{{ jdk }}"
{% endfor %}
        - profiling
        - debugger
      name: required
      stage: required

workflows:
{% if is_regular %}
  build_test:
    jobs:
      *build_test_jobs
{% endif %}
{% if is_nightly %}
  nightly:
    jobs:
      *build_test_jobs
{% endif %}
{% if is_weekly %}
  weekly:
    jobs:
      # This will rebuild a main caches with a new timestamp from a clean slate
      - build_clean_cache:
          name: build_cache_lib
          gradleTarget: shadowJar
          cacheType: lib
          collectLibs: false
      - build_clean_cache:
          name: build_cache_base
          gradleTarget: :baseTest
          cacheType: base
      - build_clean_cache:
          name: build_cache_inst
          gradleTarget: :instrumentationTest
          cacheType: inst
      - build_clean_cache:
          name: build_cache_latestdep
          gradleTarget: :instrumentationLatestDepTest
          cacheType: latestdep
      - build_clean_cache:
          name: build_cache_smoke
          gradleTarget: :smokeTest
          cacheType: smoke
      - build_clean_cache:
          name: build_cache_profiling
          gradleTarget: :profilingTest
          cacheType: profiling
{% endif %}
